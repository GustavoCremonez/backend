# TarefAi Backend

Backend para extraÃ§Ã£o automÃ¡tica de tarefas de textos de daily/reuniÃ£o, utilizando LLM (Gemini) para anÃ¡lise semÃ¢ntica e retorno estruturado em JSON.

## âœ¨ Funcionalidades
- Recebe texto de daily/transcriÃ§Ã£o e retorna tarefas feitas e a fazer, separadas por responsÃ¡vel.
- Normaliza prazos em datas quando possÃ­vel.
- Rate limiting, logging estruturado, modularizaÃ§Ã£o e seguranÃ§a.
- Pronto para produÃ§Ã£o com Docker.

## ğŸš€ Tecnologias
- Python 3.12
- FastAPI
- Uvicorn
- Gemini API (Google)
- Pydantic
- SlowAPI (rate limiting)
- python-dotenv
- dateparser

## âš™ï¸ Requisitos
- Python 3.12+
- Conta e chave de API Gemini (https://aistudio.google.com/app/apikey)
- Docker (opcional, para deploy containerizado)

## ğŸ› ï¸ InstalaÃ§Ã£o e uso local
1. Clone o repositÃ³rio e acesse a pasta:
   ```sh
   git clone <repo-url>
   cd backend
   ```
2. Crie e ative um ambiente virtual:
   ```sh
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   .venv\Scripts\activate    # Windows
   ```
3. Instale as dependÃªncias:
   ```sh
   pip install -r requirements.txt
   ```
4. Crie um arquivo `.env` na raiz com sua chave Gemini:
   ```env
   GEMINI_API_KEY=COLE_SUA_CHAVE_AQUI
   ```
5. Rode o servidor:
   ```sh
   uvicorn main:app --reload
   ```
6. Acesse a documentaÃ§Ã£o interativa em [http://localhost:8000/docs](http://localhost:8000/docs)

## ğŸ³ Uso com Docker
1. Build da imagem:
   ```sh
   docker build -t tarefai-backend .
   ```
2. Rode o container (usando seu .env):
   ```sh
   docker run --env-file .env -p 8000:8000 tarefai-backend
   ```

## ğŸ”‘ VariÃ¡veis de ambiente
- `GEMINI_API_KEY`: **ObrigatÃ³ria**. Chave da API Gemini (Google).

## ğŸ“‹ Exemplo de request/response
### Request
```json
POST /extract-tasks
{
  "texto": "Lucas disse que ontem finalizou o componente de login, mas ainda precisa revisar a integraÃ§Ã£o com o backend."
}
```
### Response
```json
[
  {
    "responsavel": "Lucas",
    "feitas": ["Finalizou o componente de login"],
    "a_fazer": [
      {
        "task": "Revisar a integraÃ§Ã£o com o backend",
        "prazo": "",
        "data_prazo": "",
        "descricao": ""
      }
    ]
  }
]
```

### Modos de extraÃ§Ã£o de tarefas

O backend suporta trÃªs modos de extraÃ§Ã£o de tarefas:

- **Gemini (nuvem):** Usa LLM da Google Gemini. Os dados sÃ£o enviados para a nuvem. Ideal para mÃ¡xima inteligÃªncia, mas pode expor dados sensÃ­veis.
- **spaCy (local):** Usa processamento de linguagem natural local com spaCy e regras. NÃ£o envia dados para a nuvem, ideal para privacidade e uso em ambientes restritos.
- **auto (hÃ­brido):** Tenta extrair tarefas com spaCy primeiro (rÃ¡pido e privativo). Se o resultado for vazio ou de baixa confianÃ§a, faz fallback automÃ¡tico para Gemini, garantindo mÃ¡xima robustez.

### Como escolher o modo

No corpo da requisiÃ§Ã£o, envie o campo `provedor` com um destes valores:
- `"spacy"` â€” forÃ§a uso local
- `"gemini"` â€” forÃ§a uso nuvem
- `"auto"` â€” (recomendado) tenta local, faz fallback para nuvem se necessÃ¡rio

Exemplo de payload:
```json
{
  "texto": "Lucas disse que ontem finalizou o componente de login, mas ainda precisa revisar a integraÃ§Ã£o com o backend.",
  "provedor": "auto"
}
```

No modo `auto`, o backend sÃ³ envia dados para a nuvem se o spaCy nÃ£o conseguir extrair tarefas com confianÃ§a.

### InstalaÃ§Ã£o do modelo spaCy para portuguÃªs

Se for usar o modo spaCy, instale o modelo de portuguÃªs (recomendado usar o modelo grande):

```bash
pip install spacy
python -m spacy download pt_core_news_lg
```

Se rodar em Docker, adicione ao Dockerfile:
```Dockerfile
RUN python -m spacy download pt_core_news_lg
```

## ğŸ§ª Testes
Execute os testes automatizados com:
```sh
python -m unittest test_data/test_extractor.py
```

## ğŸ“š DocumentaÃ§Ã£o
- Acesse `/docs` para Swagger/OpenAPI interativo.
- Modelos de entrada e saÃ­da documentados automaticamente.

## ğŸ‘¨â€ğŸ’» Contato
DÃºvidas ou sugestÃµes? Abra uma issue ou entre em contato! 